Major items in this commit
--------------------------
- Cleaned up the optimization code a little bit
- Added the ability to save and load networks, so you don't have to re-train every time

Backlog (major items):
----------------------
- Switch everything to W,b
- Add early stopping
- look into packing/deprecation
- start profiling code
- clean up the directory structure
- unit tests for demos and other functions, including gradient descent
- add documentation for demo
- write a display function in nnetutils which prints out stuff
- Consider writing a 'stacked autoencoder' class - OR! maybe add it directly to the autoencoder class? if you give it an array of hidden nodes, it will just train the deep network in a greedy, layer-wise manner? might be promising...
- Change the name of the DeepAutoencoderClassifier - it doesn't need to necessarily be a deep network. Also change the name of the variable 'sae_decay', as it doesn't need to be sparse.
- Add in sparse filtering

Ongoing:
--------
- Coding style
- Design considerations
- Application to other major datasets

Thoughts/Concerns:
------------------------
- All the optimization techniques assume X to be pre-appended with a row of ones at the top - should I just switch permanently to W,b, or keep this 'extended' data matrix style? 
- Currently have the data matrix as d x m, whereas most conventions use m x d - I should probably switch this at some point, but I think this will be a significant effort... maybe not necessary?
