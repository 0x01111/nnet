Major items in this commit
--------------------------
- Created a new deep autoencoder class which performs pre-training and fine-tuning. Seems to work well! There is definitely a significant increase in performance after fine-tuning. Haven't written any tests for it, however...

Backlog (major items):
----------------------
- clean up the optimization techniques - use scipy.optimize.minimize(), and write a function that provides both cost and grad - easier to work with - this also has the added advantage of using custom optimization techniques
- look into packing/deprecation
- start profiling code
- clean up the directory structure
- unit tests for demos and other functions, including gradient descent
- add documentation for demo
- write a display function in nnetutils which prints out stuff
- Consider writing a 'stacked autoencoder' class - OR! maybe add it directly to the autoencoder class? if you give it an array of hidden nodes, it will just train the deep network in a greedy, layer-wise manner? might be promising...
- Change the name of the DeepAutoencoderClassifier - it doesn't need to necessarily be a deep network. Also change the name of the variable 'sae_decay', as it doesn't need to be sparse.

Ongoing:
--------
- Coding style
- Design considerations
- Application to other major datasets

Thoughts/Concerns:
------------------------
- All the optimization techniques assume X to be pre-appended with a row of ones at the top - should I just switch permanently to W,b, or keep this 'extended' data matrix style? 
- Currently have the data matrix as d x m, whereas most conventions use m x d - I should probably switch this at some point, but I think this will be a significant effort... maybe not necessary?
