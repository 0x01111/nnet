This commit:
------------
- implement gradient_descent, momentum, and improved momentum, and a function in NeuralNetCore which can plot the error curves 

Backlog:
-------
- start working through additional sections of Andrew Ng's code and 
- start training a deep network using greedy layer-wise training
- autodiff + theano
- look into packing/deprecation
- Apply a sparse autoencoder to london scikits data
- start profiling code

Notes:
-------
- All the optimization techniques assume X to be pre-appended with a row of ones at the top - should I just switch permanently to W,b, or keep this 'joint' data matrix? 
