This document is intended to report some the results and benchmarks I obtained from 
using my implementation of neural networks on the Stanford UFLDL exercises. Most of the
results so far seem to agree with those reported in the tutorial. The speeds were also
closer to those reported after installing and linking numpy with OpenBLAS, which made
the matrix multiplications significantly faster (~0.2s vs ~0.06s for multiplying two
dense 1000 x 1000 sized random matrices). So far, results are reported for:

MNIST_demo.py
STL_demo.py

1. MNIST_demo.py

MNIST classification using Softmax regression

Data:
-----
Number of samples for training: 60000
Number of samples for testing: 10000

Parameters:
-----------
Input feature size: 784
Output dimension: 10
Decay term: 0.0001
Optimization method: L-BFGS
Max iterations: 400

Performance:
------------
Accuracy: 92.63%
[Finished in 151.2s]

2. STL_demo.py

Self-taught learning demo

Data:
-----
Number of samples for training: 30596
Number of samples for testing: 15298

Part 1: Softmax regression on raw pixels

Parameters:
-----------
Input feature size: 784
Output dimension: 5
Decay term: 0.0001
Optimization method: L-BFGS
Max iterations: 400

Performance:
------------
Accuracy: 96.8%

Part 2: Softmax regression on learned features via autoencoders

Parameters:
-----------
- Autoencoder -
Input feature size: 784
Number of hidden units: 200
Decay term: 0.003
Sparsity term: 0.01
Beta: 3
Optimization method: L-BFGS
Max iterations: 400

- Softmax -
Input feature size: 200
Output dimension: 5
Decay term: 0.0001
Optimization method: L-BFGS
Max iterations: 400

Performance:
------------
Accuracy: 98.2%
[Finished in s]
